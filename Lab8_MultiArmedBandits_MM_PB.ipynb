{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab8_MultiArmedBandits_MM_PB.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "hWwXip_XEYxq"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.1 64-bit ('ml4e-multi-armed-bandit': conda)",
      "metadata": {
        "interpreter": {
          "hash": "c1351bb44cbf938b6cf57e050ae12425d6167d72239c28bac28cb70304bb5017"
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2puEAzUh5WRp"
      },
      "source": [
        "<center>\n",
        "    <h1>Machine Learning for Economists Winter 2020/21</h1>\n",
        "    <h3>Februrary 5th, 2021</h3>\n",
        "</center>\n",
        "\n",
        "# Lab 8: Multi-Armed Bandits\n",
        "_By Mario Müller & Pascal Budner_\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/8/82/Las_Vegas_slot_machines.jpg\" width=\"300px\" />\n",
        "\n",
        "Consider a two-armed bandit problem with 1000 sequential draws where rewards are either 0 or 1. For arm a, P (y = 1|D = a) ≡ pa denotes the probability of the reward being equal to 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNOazDl-QEQr",
        "outputId": "71013fc4-d419-46a8-9ef7-a267390a41d2"
      },
      "source": [
        "# first, importing some libraries\n",
        "from scipy import stats # used to draw custom random distribution\n",
        "import statsmodels.stats.api as sms # used to calculate power stuff\n",
        "import seaborn as sns # used for visualization purposes\n",
        "import matplotlib.pyplot as plt # used for visualization purposes\n",
        "import numpy as np # used to argmax array\n",
        "import random # used to draw random number\n",
        "\n",
        "# set parameters\n",
        "T = 1000 # number of sequentials draws during a single Multi-armed bandit game\n",
        "no_of_simulations = 2000 # number of simulation executions\n",
        "\n",
        "# seaborn theme\n",
        "sns.set_theme(style=\"darkgrid\", color_codes=True)\n",
        "sns.set_context('paper')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvWTzWPJ5IYz"
      },
      "source": [
        "## 1. Classic Randomized Experiment\n",
        "Suppose that p1 = .05 and p2 = .1. If you ran a classic randomized experiment, how many observations would you need to sample into each treatment arm to detect a difference between arm 1 and arm 2 at a two-sided significance level of .99 and with power of .8? Adjust the code from the lecture notes to find out. Also, what would be the regret?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpDaIYeZ_B-m"
      },
      "source": [
        "### Calculate the Number of Needed Observations\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_DJXcX55Hx7",
        "outputId": "c551377f-b7b7-48ae-e849-b3f637a349e4"
      },
      "source": [
        "p_1 = 0.05\n",
        "p_2 = 0.1\n",
        "significane_lvl = 0.01\n",
        "power = 0.8\n",
        "\n",
        "# First, calculate the number of observations we need to detect \n",
        "# a difference between both arms with power of 0.8 and two-sided\n",
        "# significance level of 0.99\n",
        "es = sms.proportion_effectsize(p_1, p_2) # calculate effect size comparing two proportions\n",
        "num_of_needed_obs = sms.NormalIndPower().solve_power(es, power=power, alpha=significane_lvl, ratio=1) # returns number of observations per sample\n",
        "print(\"We need to sample {:.0f} obversations per treatment arm.\".format(num_of_needed_obs))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We need to sample 631 obversations per treatment arm.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuQ_EQPA_K_N"
      },
      "source": [
        "### Calculate the Regret\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQvIkoBZ_NSA",
        "outputId": "daab09b6-2fa3-4abc-8e54-c4c87a4b1819"
      },
      "source": [
        "# doing a randomized experiment we would in num_of_needed_obs times \n",
        "# play the wrong arm (i.e., arm 1). Hence, we miss the p_2 - p_1 chance\n",
        "# of getting a reward\n",
        "regret = (p_2 - p_1) * num_of_needed_obs\n",
        "print(\"The regret is {:.0f} before we realize which arm is better at a power of {} and p<{}.\".format(regret, power, significane_lvl))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The regret is 32 before we realize which arm is better at a power of 0.8 and p<0.01.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWwXip_XEYxq"
      },
      "source": [
        "## 2. Introducing the Multi-Armed Bandit Setting\n",
        "Instead, you decide to conduct experimentation using a multi-armed bandit setting. To find out how the bandit setting will fare in comparison to the randomized experiment, we simulate the bandit setting many times. The simulation requires\n",
        "-  A function that takes random draws from the rewards distribution\n",
        "- A rule that prescribes which arm we try next. As a simple benchmark rule, implement greedy action selection as discussed in the lecture notes.\n",
        "\n",
        "Simulate cumulative rewards over 1000 draws by setting up a loop that\n",
        "\n",
        "1. prescribes which arm to try next (via some action selection rule)\n",
        "2. pulls the arm\n",
        "3. and stores the realized reward\n",
        "\n",
        "Note that due to the stochastic nature of payoffs, cumulative rewards will look different each time you run the loop. Run the loop 2000 times and plot the average cumulative reward. (Hint: You need to write an outer loop over the number of simulations, and you need to store cumulative rewards each round.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "La_VweWDFIXH"
      },
      "source": [
        "# this function draws the reward from a discrete random distribution\n",
        "# the parameter p specifies the proability to retrieve the reward\n",
        "def draw_from_distribution(p):\n",
        "  reward_distribution = stats.rv_discrete(values=((0, 1),(1-p, p)))\n",
        "  return reward_distribution.rvs(size=1)[0]\n",
        "\n",
        "# select arm based on highest expected reward\n",
        "def greedy_action_selector(expected_reward_per_arm):\n",
        "  if min(expected_reward_per_arm) == max(expected_reward_per_arm): # we have no superior arm found yet\n",
        "    d_t = random.randint(0,len(expected_reward_per_arm)-1) # pick randomly one of the arms\n",
        "  else:\n",
        "    d_t = np.argmax(expected_reward_per_arm) # pick arm with highgest expected reward\n",
        "  return d_t\n",
        "\n",
        "def play_bandit_game(num_of_draws, p_array, action_selector):\n",
        "  # we store those variables in an array for easier code\n",
        "  p = p_array # reward probability for arms\n",
        "  cumulative_rewards = [0.0] * len(p_array) # init to 0.0 per arm (float)\n",
        "  number_of_times_arm_chosen = [0.0] * len(p_array) # init to 0 per arm\n",
        "  expected_reward_per_arm = [0.0] * len(p_array) # init to 0 per arm\n",
        "  \n",
        "  # play bandit game num_of_draws times\n",
        "  for i in range(num_of_draws):\n",
        "    # select which arm to choose, by using the one with the highest expected revenue\n",
        "    d_t = action_selector(expected_reward_per_arm)\n",
        "    \n",
        "    # play arm, retrieve reward, and update variables\n",
        "    reward = draw_from_distribution(p[d_t])\n",
        "#    print(\"#{} – Played arm {} and retrieved a reward of {}.\".format(i+1, d_t+1, reward))\n",
        "    cumulative_rewards[d_t] += reward # update cumulative reward\n",
        "    number_of_times_arm_chosen[d_t] += 1 # increase number of times arm played\n",
        "\n",
        "    # update expected reward for selected arm\n",
        "    expected_reward_per_arm[d_t] = cumulative_rewards[d_t] / number_of_times_arm_chosen[d_t]\n",
        "  return sum(cumulative_rewards)\n",
        "\n",
        "def simulate_bandit_game(p_array, action_selector):\n",
        "  # play the mutli-armed bandit game no_of_simulations times and save each received reward\n",
        "  rewards = []\n",
        "\n",
        "  # execute the bandit game no_of_simulations times\n",
        "  for iteration in range(no_of_simulations):\n",
        "    rewards.append(play_bandit_game(T, p_array, greedy_action_selector))\n",
        "  return rewards\n",
        "\n",
        "# play the bandits game with greedy action selection\n",
        "p_array = [p_1, p_2]\n",
        "rewards = simulate_bandit_game(p_array, greedy_action_selector)\n",
        "\n",
        "# visualize results\n",
        "plt.figure(figsize=(6,5))\n",
        "graph = sns.scatterplot(data=rewards)\n",
        "graph.axes.axhline(max(p_array) * T, color='green', linestyle='--') # expected reward if we would always choose the best arm\n",
        "graph.axes.axhline(sum(rewards)/len(rewards), color='orange', linestyle='--') # average reward of our sample\n",
        "plt.xlabel('Simulation Iteration');\n",
        "plt.ylabel('Cumulative Reward after {} draws'.format(T));\n",
        "plt.title('Multi-Armed Bandits with Greedy Action Selection');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHnR4GZVQaRb"
      },
      "source": [
        "## 3. ε-greddy Algorithm Implementation\n",
        "Write a function epsGreedy that implements the ε-greedy algorithm. Compare the performance of\n",
        "the ε-greedy algorithm to the greedy algorithm from part 2 for ε ∈ {.1, .3}."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhVGqzNOQ-RJ"
      },
      "source": [
        "# select arm based on highest expected reward\n",
        "def epsilon_greedy_action_selector(expected_reward_per_arm, epsilon):\n",
        "  random_pick = random.randint(0,len(expected_reward_per_arm)-1)\n",
        "\n",
        "  # draw random number (uniformely distributed between [0.0,1.0])\n",
        "  if random.random() <= epsilon: # if epsilon is smaller, then we just pick a random guess\n",
        "    return random_pick\n",
        "\n",
        "  # otherwise we apply our greedy action selection algorithm \n",
        "  return greedy_action_selector(expected_reward_per_arm)\n",
        "\n",
        "# visualize results\n",
        "fig, axes = plt.subplots(1, 3, sharey=True, figsize=(20, 5))\n",
        "fig.suptitle('Multi-Armed Bandit with ε-Greedy Action Selection')\n",
        "\n",
        "p_array = [p_1, p_2]\n",
        "\n",
        "# greedy action selection\n",
        "rewards = simulate_bandit_game(p_array, lambda x: greedy_action_selector(x))\n",
        "sns.scatterplot(ax=axes[0], data=rewards)\n",
        "axes[0].axhline(max(p_array) * T, color='green', linestyle='--') # expected reward if we would always choose the best arm\n",
        "axes[0].axhline(sum(rewards)/len(rewards), color='orange', linestyle='--') # average reward of our sample\n",
        "axes[0].set_title('Standard Algorithm'.format(epsilon))\n",
        "axes[0].set_ylabel('Cumulative Reward after {} draws'.format(T))\n",
        "axes[0].set_xlabel('Simulation Iteration')\n",
        "\n",
        "\n",
        "# first e-greedy game\n",
        "epsilon = 0.1\n",
        "rewards = simulate_bandit_game(p_array, lambda x: epsilon_greedy_action_selector(x, epsilon))\n",
        "sns.scatterplot(ax=axes[1], data=rewards)\n",
        "axes[1].axhline(max(p_array) * T, color='green', linestyle='--') # expected reward if we would always choose the best arm\n",
        "axes[1].axhline(sum(rewards)/len(rewards), color='orange', linestyle='--') # average reward of our sample\n",
        "axes[1].set_title('ε = {}'.format(epsilon))\n",
        "axes[1].set_ylabel('Cumulative Reward after {} draws'.format(T))\n",
        "axes[1].set_xlabel('Simulation Iteration')\n",
        "\n",
        "# second e-greedy game\n",
        "epsilon = 0.3\n",
        "rewards = simulate_bandit_game(p_array, lambda x: epsilon_greedy_action_selector(x, epsilon))\n",
        "sns.scatterplot(ax=axes[2], data=rewards)\n",
        "axes[2].axhline(max(p_array) * T, color='green', linestyle='--') # expected reward if we would always choose the best arm\n",
        "axes[2].axhline(sum(rewards)/len(rewards), color='orange', linestyle='--') # average reward of our sample\n",
        "axes[2].set_title('ε = {}'.format(epsilon))\n",
        "axes[2].set_ylabel('Cumulative Reward after {} draws'.format(T))\n",
        "axes[2].set_xlabel('Simulation Iteration')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMVMiXyFic-m"
      },
      "source": [
        "## 4. Sensitivity of ε-greddy Algorithm\n",
        "Explore how the results for the epsilon greedy algorithm change when the reward distributions for\n",
        "the two arms are very different, in particular, set p2 = .5, and compare again ε = .1 to ε = .3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AO8XTF1niktn"
      },
      "source": [
        "p_2_new = 0.5\n",
        "p_array = [p_1, p_2_new]\n",
        "\n",
        "# visualize results\n",
        "fig, axes = plt.subplots(1, 3, sharey=True, figsize=(20, 5))\n",
        "fig.suptitle('Multi-Armed Bandit with ε-Greedy Action Selection')\n",
        "\n",
        "# greedy action selection\n",
        "rewards = simulate_bandit_game(p_array, lambda x: greedy_action_selector(x))\n",
        "sns.scatterplot(ax=axes[0], data=rewards)\n",
        "axes[0].axhline(max(p_array) * T, color='green', linestyle='--') # expected reward if we would always choose the best arm\n",
        "axes[0].axhline(sum(rewards)/len(rewards), color='orange', linestyle='--') # average reward of our sample\n",
        "axes[0].set_title('Standard Algorithm'.format(epsilon))\n",
        "axes[0].set_ylabel('Cumulative Reward after {} draws'.format(T))\n",
        "axes[0].set_xlabel('Simulation Iteration')\n",
        "\n",
        "\n",
        "# first e-greedy game\n",
        "epsilon = 0.1\n",
        "rewards = simulate_bandit_game(p_array, lambda x: epsilon_greedy_action_selector(x, epsilon))\n",
        "sns.scatterplot(ax=axes[1], data=rewards)\n",
        "axes[1].axhline(max(p_array) * T, color='green', linestyle='--') # expected reward if we would always choose the best arm\n",
        "axes[1].axhline(sum(rewards)/len(rewards), color='orange', linestyle='--') # average reward of our sample\n",
        "axes[1].set_title('ε = {}'.format(epsilon))\n",
        "axes[1].set_ylabel('Cumulative Reward after {} draws'.format(T))\n",
        "axes[1].set_xlabel('Simulation Iteration')\n",
        "\n",
        "# second e-greedy game\n",
        "epsilon = 0.3\n",
        "rewards = simulate_bandit_game(p_array, lambda x: epsilon_greedy_action_selector(x, epsilon))\n",
        "sns.scatterplot(ax=axes[2], data=rewards)\n",
        "axes[2].axhline(max(p_array) * T, color='green', linestyle='--') # expected reward if we would always choose the best arm\n",
        "axes[2].axhline(sum(rewards)/len(rewards), color='orange', linestyle='--') # average reward of our sample\n",
        "axes[2].set_title('ε = {}'.format(epsilon))\n",
        "axes[2].set_ylabel('Cumulative Reward after {} draws'.format(T))\n",
        "axes[2].set_xlabel('Simulation Iteration')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRzaxxpwj0Ug"
      },
      "source": [
        "## 5. Increasing of the Number of Bandits\n",
        "Explore how results change when you increase the number of bandits by using p1 =.05, p2 =.1, p3 =.01, p4 = .11, p5 = .4. Compare again ε = .1 to ε = .3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbpag_2bkBEz"
      },
      "source": [
        "p_array = [0.05, 0.1, 0.01, 0.11, 0.4]\n",
        "\n",
        "# visualize results\n",
        "fig, axes = plt.subplots(1, 3, sharey=True, figsize=(20, 5))\n",
        "fig.suptitle('Multi-Armed Bandit with ε-Greedy Action Selection')\n",
        "\n",
        "# greedy action selection\n",
        "rewards = simulate_bandit_game(p_array, lambda x: greedy_action_selector(x))\n",
        "sns.scatterplot(ax=axes[0], data=rewards)\n",
        "axes[0].axhline(max(p_array) * T, color='green', linestyle='--') # expected reward if we would always choose the best arm\n",
        "axes[0].axhline(sum(rewards)/len(rewards), color='orange', linestyle='--') # average reward of our sample\n",
        "axes[0].set_title('Standard Algorithm'.format(epsilon))\n",
        "axes[0].set_ylabel('Cumulative Reward after {} draws'.format(T))\n",
        "axes[0].set_xlabel('Simulation Iteration')\n",
        "\n",
        "\n",
        "# first e-greedy game\n",
        "epsilon = 0.1\n",
        "rewards = simulate_bandit_game(p_array, lambda x: epsilon_greedy_action_selector(x, epsilon))\n",
        "sns.scatterplot(ax=axes[1], data=rewards)\n",
        "axes[1].axhline(max(p_array) * T, color='green', linestyle='--') # expected reward if we would always choose the best arm\n",
        "axes[1].axhline(sum(rewards)/len(rewards), color='orange', linestyle='--') # average reward of our sample\n",
        "axes[1].set_title('ε = {}'.format(epsilon))\n",
        "axes[1].set_ylabel('Cumulative Reward after {} draws'.format(T))\n",
        "axes[1].set_xlabel('Simulation Iteration')\n",
        "\n",
        "# second e-greedy game\n",
        "epsilon = 0.3\n",
        "rewards = simulate_bandit_game(p_array, lambda x: epsilon_greedy_action_selector(x, epsilon))\n",
        "sns.scatterplot(ax=axes[2], data=rewards)\n",
        "axes[2].axhline(max(p_array) * T, color='green', linestyle='--') # expected reward if we would always choose the best arm\n",
        "axes[2].axhline(sum(rewards)/len(rewards), color='orange', linestyle='--') # average reward of our sample\n",
        "axes[2].set_title('ε = {}'.format(epsilon))\n",
        "axes[2].set_ylabel('Cumulative Reward after {} draws'.format(T))\n",
        "axes[2].set_xlabel('Simulation Iteration')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}